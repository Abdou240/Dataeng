Here‚Äôs a simplified breakdown of your end-to-end data pipeline:  

### **1. Data Ingestion (Batch Processing)**  
- **Google Cloud Storage (GCS)** ‚Üí Stores raw data.  
- **Terraform** ‚Üí Provisions infrastructure (GCS, BigQuery, Airflow).  
- **Docker** ‚Üí Containerizes the pipeline.  
- **Apache Airflow** ‚Üí Orchestrates batch processing.  

### **2. Data Processing**  
- **PySpark** ‚Üí Cleans and processes raw data.  
- **Airflow DAG** ‚Üí Moves processed data to BigQuery.  

### **3. Data Warehousing**  
- **Google BigQuery** ‚Üí Stores structured, partitioned, and clustered data.  

### **4. Data Transformation**  
- **dbt (Data Build Tool)** ‚Üí Applies SQL-based transformations on BigQuery.  

### **5. Data Visualization**  
- **Power BI** ‚Üí Connects to BigQuery and creates dashboards.  

### **6. Infrastructure as Code**  
- **Terraform** ‚Üí Deploys and manages the infrastructure.  

Here‚Äôs a **detailed explanation** of each step in your **end-to-end data pipeline** using GCP and modern data tools:

---

## **1. Data Ingestion (Batch Processing)**
This is the first step where raw data is collected and stored before processing.

- **Google Cloud Storage (GCS)**:  
  - Acts as a **data lake** where raw files (CSV, JSON, Parquet, etc.) are stored.  
  - You can upload data manually or automate it via scripts/API.  

- **Terraform (Infrastructure as Code - IaC)**:  
  - Automates the provisioning of GCP resources like **GCS, BigQuery, and Airflow**.  
  - Ensures infrastructure is reproducible and version-controlled.  

- **Docker (Containerization)**:  
  - Packages applications and dependencies into **containers** for consistency across environments.  
  - Helps deploy Airflow, PySpark jobs, and dbt in an isolated environment.  

- **Apache Airflow (Workflow Orchestration)**:  
  - Manages and schedules batch jobs using **DAGs (Directed Acyclic Graphs)**.  
  - Defines steps like:  
    1. Extract data from **GCS**.  
    2. Process it using **PySpark**.  
    3. Load it into **BigQuery**.  

---

## **2. Data Processing**
After ingestion, data needs cleaning and transformation before storage.

- **PySpark (Batch Processing Framework)**:  
  - Runs distributed processing for large datasets.  
  - Performs data cleaning, filtering, aggregation, and transformations.  
  - Converts raw data into structured format before loading into BigQuery.  

- **Airflow DAGs (Task Automation)**:  
  - Automates PySpark processing.  
  - Moves processed data from GCS to BigQuery in an **ETL pipeline**.  

---

## **3. Data Warehousing**
Once processed, data is stored in a structured format for easy querying.

- **Google BigQuery (Cloud Data Warehouse)**:  
  - Stores **processed, structured data**.  
  - Supports **partitioning (by date)** and **clustering (by category)** to improve performance.  
  - Handles **large-scale analytics** with SQL queries.  

---

## **4. Data Transformation**
After storing the data, transformations are applied to prepare it for analytics.

- **dbt (Data Build Tool)**:  
  - Defines transformations using **SQL models**.  
  - Creates **views and tables** in BigQuery for analysis.  
  - Automates **incremental updates** to optimize queries.  

Example:  
- Raw table: **user_clicks_raw**  
- Transformed table: **daily_user_activity_summary**  

---

## **5. Data Visualization**
Now that the data is structured and transformed, it‚Äôs ready for reporting.

- **Power BI (Business Intelligence Tool)**:  
  - Connects to **BigQuery** for real-time dashboards.  
  - Creates **two types of visualizations**:  
    1. **Categorical Distribution Graph** (e.g., Top 10 products by sales).  
    2. **Time Series Graph** (e.g., Sales growth over time).  
  - Ensures interactive and easy-to-read dashboards.  

---

## **6. Infrastructure as Code (Automation & Deployment)**
To keep the pipeline scalable and reproducible, infrastructure is automated.

- **Terraform**:  
  - Deploys **GCS, BigQuery, Airflow, dbt, and other resources** with a single script.  
  - Ensures everything is **version-controlled and repeatable**.  

---

## **Final Workflow Summary**
1. **Raw Data** ‚Üí Stored in **GCS**.  
2. **Airflow DAG** ‚Üí Triggers **PySpark** processing.  
3. **PySpark** ‚Üí Cleans and loads data into **BigQuery**.  
4. **dbt** ‚Üí Transforms data for analytics.  
5. **Power BI** ‚Üí Visualizes insights for decision-making.  

---

This **end-to-end pipeline** ensures **automated, scalable, and efficient data processing** using GCP tools.


### **Step 1: Data Ingestion (Batch Processing) - Detailed Breakdown**  
The first step is to **ingest raw data** and store it in a **data lake** for further processing. This step ensures that data is collected, stored securely, and is available for processing in a structured way.

---

## **üìå Objective of This Step**  
We need to collect raw data from a source, store it in **Google Cloud Storage (GCS)**, and automate infrastructure deployment using **Terraform**.

---

## **üìÇ Example Use Case: E-commerce Sales Data**  
Let's assume we are working with an **e-commerce dataset** that contains **customer transactions**. The data is generated by the online store daily in CSV format.  
   
**Example raw data file (`sales_data_2025-03-25.csv`)**:  
| Order_ID | Customer_ID | Product | Category | Price | Quantity | Timestamp           |  
|----------|------------|---------|----------|-------|----------|---------------------|  
| 1001     | 500        | Laptop  | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  
| 1002     | 501        | Phone   | Electronics | 800   | 2        | 2025-03-25 09:00:00 |  

---

## **üõ†Ô∏è Technologies Used & Why?**  

| Technology  | Purpose  | How We Use It?  |  
|-------------|----------|----------------|  
| **Google Cloud Storage (GCS)** | Stores raw data | Upload CSV files to a **dedicated bucket** for processing |  
| **Terraform (IaC)** | Automates cloud resource creation | Creates the GCS bucket and IAM roles programmatically |  
| **Docker** | Ensures consistent environment | Runs Airflow in a **containerized setup** |  
| **Apache Airflow** | Orchestrates the pipeline | Automates file ingestion from a local system to GCS |  
| **Bash (Shell Scripting)** | Uploads files programmatically | Automates file transfers to GCS |  

---

## **üõ†Ô∏è Step-by-Step Implementation**

### **1Ô∏è‚É£ Create Google Cloud Storage (GCS) Bucket**  
Before uploading files, we need a **GCS bucket** to store raw data.

- **Terraform Script to Create GCS Bucket (`gcs.tf`)**:
```hcl
resource "google_storage_bucket" "raw_data_bucket" {
  name     = "ecommerce-raw-data"
  location = "US"
  storage_class = "STANDARD"
}
```
‚úÖ **What this does?**  
- Creates a **GCS bucket** named `"ecommerce-raw-data"`.  
- Sets the **storage class** to `"STANDARD"` for cost efficiency.  

---

### **2Ô∏è‚É£ Upload Data to GCS (Using Bash Script & Airflow)**  
Once the bucket is created, we need to **upload raw CSV files**.

- **Bash Script to Upload Data (`upload_data.sh`)**:
```bash
#!/bin/bash
BUCKET_NAME="ecommerce-raw-data"
FILE_PATH="data/sales_data_2025-03-25.csv"

# Upload file to GCS
gsutil cp $FILE_PATH gs://$BUCKET_NAME/raw/
echo "File uploaded successfully to GCS: $BUCKET_NAME"
```
‚úÖ **What this does?**  
- Uses **`gsutil cp`** to copy a local file (`sales_data_2025-03-25.csv`) to GCS.  
- Stores the file inside a `"raw/"` directory in GCS.  

---

### **3Ô∏è‚É£ Automate File Upload with Apache Airflow**  
We don‚Äôt want to manually run the script daily. Instead, we use **Airflow** to automate the process.

- **Airflow DAG to Upload Data (`upload_to_gcs.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

# Define default arguments
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 25),
    'retries': 1
}

# Define the DAG
dag = DAG(
    dag_id='upload_sales_data_to_gcs',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

# Define the task
upload_task = BashOperator(
    task_id='upload_csv_to_gcs',
    bash_command='bash /opt/airflow/scripts/upload_data.sh',
    dag=dag
)

upload_task
```
‚úÖ **What this does?**  
- **Schedules** the upload task to run **daily**.  
- **Triggers the Bash script** (`upload_data.sh`) to copy files to GCS.  

---

### **4Ô∏è‚É£ Verify Data in GCS**  
After execution, check if the file is uploaded:  
```bash
gsutil ls gs://ecommerce-raw-data/raw/
```
üîπ Expected Output:  
```
gs://ecommerce-raw-data/raw/sales_data_2025-03-25.csv
```
---

## **üéØ Summary of Step 1**
‚úîÔ∏è **Created a GCS bucket** for raw data storage using **Terraform**.  
‚úîÔ∏è **Wrote a Bash script** to upload CSV files to **GCS**.  
‚úîÔ∏è **Automated the upload process** using **Apache Airflow DAGs**.  
‚úîÔ∏è **Ensured repeatability** using **Docker** and **Terraform**.  

---

## **Next Steps**
‚úÖ Now that the raw data is in GCS, the next step is **processing it using PySpark** before loading it into **BigQuery**. Let me know if you want a similar detailed breakdown for the next step!

If we want to **fully automate Step 1**, including **Terraform deployment, data upload, and orchestration**, we need to containerize the entire process using **Docker**. Here‚Äôs how we can achieve that:

---

# **üìå High-Level Overview**
We will create a **Dockerized environment** that automates:
1. **Infrastructure deployment** using Terraform.  
2. **File upload to GCS** using a Bash script.  
3. **Orchestration** using Apache Airflow to trigger the upload task daily.  

---

## **üìÇ Folder Structure**
```
data-pipeline/
‚îÇ‚îÄ‚îÄ airflow/                      # Airflow-related files
‚îÇ   ‚îú‚îÄ‚îÄ dags/                     # DAGs for automation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ upload_to_gcs.py      # DAG to automate file upload
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Airflow Docker setup
‚îÇ‚îÄ‚îÄ terraform/                     # Terraform files
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                    # Terraform script to create resources
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                # Terraform variables
‚îÇ   ‚îú‚îÄ‚îÄ terraform.sh                # Shell script to apply Terraform
‚îÇ‚îÄ‚îÄ scripts/                        # Helper scripts
‚îÇ   ‚îú‚îÄ‚îÄ upload_data.sh              # Script to upload data to GCS
‚îÇ‚îÄ‚îÄ docker-compose.yml               # Docker Compose to run everything
‚îÇ‚îÄ‚îÄ .env                             # Environment variables
```

---

# **üõ†Ô∏è Step-by-Step Implementation**

### **1Ô∏è‚É£ Write the Terraform Configuration**
Create the **Terraform script** (`terraform/main.tf`) to set up GCS:

```hcl
provider "google" {
  project = var.project_id
  region  = var.region
}

resource "google_storage_bucket" "raw_data_bucket" {
  name     = "ecommerce-raw-data"
  location = var.region
  storage_class = "STANDARD"
}
```
‚úÖ **This creates a GCS bucket in GCP.**  

---

### **2Ô∏è‚É£ Automate Terraform Deployment**
Create a **shell script** (`terraform/terraform.sh`) to automate Terraform execution:

```bash
#!/bin/bash
echo "Initializing Terraform..."
terraform init

echo "Applying Terraform..."
terraform apply -auto-approve

echo "Terraform deployment completed."
```
‚úÖ **This script automatically initializes and applies Terraform.**  

---

### **3Ô∏è‚É£ Automate Data Upload to GCS**
Create a **Bash script** (`scripts/upload_data.sh`) to upload files:

```bash
#!/bin/bash
BUCKET_NAME="ecommerce-raw-data"
FILE_PATH="/opt/airflow/data/sales_data_$(date +%Y-%m-%d).csv"

echo "Uploading data to GCS..."
gsutil cp $FILE_PATH gs://$BUCKET_NAME/raw/

echo "File successfully uploaded!"
```
‚úÖ **This uploads a CSV file with the current date to GCS.**  

---

### **4Ô∏è‚É£ Create Airflow DAG for Automation**
Create an **Airflow DAG** (`airflow/dags/upload_to_gcs.py`):

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 25),
    'retries': 1
}

dag = DAG(
    dag_id='upload_sales_data_to_gcs',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

upload_task = BashOperator(
    task_id='upload_csv_to_gcs',
    bash_command='bash /opt/airflow/scripts/upload_data.sh',
    dag=dag
)

upload_task
```
‚úÖ **This DAG schedules the upload task daily.**  

---

### **5Ô∏è‚É£ Create Dockerfile for Airflow**
Create an **Airflow Dockerfile** (`airflow/Dockerfile`):

```dockerfile
FROM apache/airflow:2.5.1

USER root

# Install Google Cloud CLI for GCS operations
RUN apt-get update && apt-get install -y google-cloud-sdk

# Set working directory
WORKDIR /opt/airflow

# Copy scripts and DAGs
COPY dags /opt/airflow/dags
COPY scripts /opt/airflow/scripts
COPY terraform /opt/airflow/terraform

# Change permissions
RUN chmod +x /opt/airflow/scripts/upload_data.sh
RUN chmod +x /opt/airflow/terraform/terraform.sh

# Install dependencies
RUN pip install apache-airflow-providers-google

# Start Airflow
CMD ["airflow", "scheduler"]
```
‚úÖ **This image sets up Airflow with Google Cloud SDK and required scripts.**  

---

### **6Ô∏è‚É£ Define Docker Compose to Run Everything**
Create a **Docker Compose file** (`docker-compose.yml`) to run all components:

```yaml
version: '3.8'
services:
  terraform:
    image: hashicorp/terraform:latest
    container_name: terraform
    working_dir: /opt/airflow/terraform
    volumes:
      - ./terraform:/opt/airflow/terraform
    entrypoint: ["/bin/sh", "-c", "./terraform.sh"]

  airflow:
    build: ./airflow
    container_name: airflow
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcp-credentials.json
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./terraform:/opt/airflow/terraform
    ports:
      - "8080:8080"
    depends_on:
      - terraform
```
‚úÖ **This setup ensures:**  
- **Terraform runs first** to create infrastructure.  
- **Airflow starts automatically** and schedules the upload task.  

---

### **7Ô∏è‚É£ Run Everything**
Now, simply run the following commands:

```bash
# Start the whole pipeline
docker-compose up --build
```
---

## **‚úÖ Final Workflow**
1Ô∏è‚É£ **Terraform runs inside Docker** ‚Üí Creates GCS bucket.  
2Ô∏è‚É£ **Airflow container starts** and schedules data upload.  
3Ô∏è‚É£ **Airflow executes the DAG** ‚Üí Calls `upload_data.sh`.  
4Ô∏è‚É£ **Bash script uploads CSV files to GCS** automatically.  

---

## **üöÄ Advantages of This Setup**
‚úî **Fully automated** ‚Üí No manual steps needed.  
‚úî **Containerized** ‚Üí Works consistently across machines.  
‚úî **Infrastructure as Code (IaC)** ‚Üí Easily reproducible.  
‚úî **Orchestrated** ‚Üí Airflow ensures proper scheduling.  

---

This **Dockerized data ingestion process** ensures a **scalable, repeatable, and automated pipeline** for storing raw data in GCS. 

# **üìå Step 2: Data Processing (Batch Processing with PySpark & dbt)**  

Now that raw data is stored in **Google Cloud Storage (GCS)**, we need to **process and clean** it before loading it into **Google BigQuery (BQ)**. This ensures that data is structured, optimized, and ready for analytics.

---

## **üéØ Objective of Step 2**  
1. **Extract** raw data from GCS.  
2. **Clean & transform** it using **PySpark**.  
3. **Load the processed data** into **Google BigQuery (BQ)**.  
4. **Use dbt (Data Build Tool)** for further transformations inside BigQuery.  

---

## **üìÇ Example Use Case: E-commerce Sales Data**  
The raw data (`sales_data_YYYY-MM-DD.csv`) from Step 1 has the following issues:  
‚úÖ **Missing values** (e.g., some customers don't have a Customer_ID).  
‚úÖ **Duplicate records** (e.g., repeated transactions).  
‚úÖ **Inconsistent formatting** (e.g., date columns have mixed formats).  
‚úÖ **Unnecessary columns** that won‚Äôt be used in analytics.  

**Example Raw Data (Before Processing)**:
| Order_ID | Customer_ID | Product  | Category    | Price | Quantity | Timestamp           |  
|----------|------------|----------|------------|-------|----------|---------------------|  
| 1001     | 500        | Laptop   | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  
| 1002     | NULL       | Phone    | Electronics | 800   | 2        | 2025-03-25 09:00:00 |  
| 1003     | 500        | Laptop   | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  

**Example Processed Data (After PySpark Cleaning & Transformation)**:
| Order_ID | Customer_ID | Product  | Category    | Total_Price | Order_Date  |  
|----------|------------|----------|------------|------------|------------|  
| 1001     | 500        | Laptop   | Electronics | 1200       | 2025-03-25 |  
| 1002     | Unknown    | Phone    | Electronics | 1600       | 2025-03-25 |  

---

## **üõ†Ô∏è Technologies Used & Why?**

| Technology  | Purpose  | How We Use It?  |  
|-------------|----------|----------------|  
| **PySpark** | Big data processing | Reads raw data from GCS, cleans it, and transforms it |  
| **Google BigQuery (BQ)** | Cloud data warehouse | Stores the processed data for analytics |  
| **dbt (Data Build Tool)** | Data transformation inside BQ | Performs final modeling and aggregations |  
| **Apache Airflow** | Orchestration | Automates the PySpark job and dbt execution |  

---

# **üõ†Ô∏è Step-by-Step Implementation**

### **1Ô∏è‚É£ Read Raw Data from GCS Using PySpark**  
We'll create a PySpark job to read data from GCS.

üìå **PySpark Script (`scripts/process_sales_data.py`)**:
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, sum, to_date

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("SalesDataProcessing") \
    .getOrCreate()

# Define GCS source and BQ destination
gcs_path = "gs://ecommerce-raw-data/raw/sales_data_2025-03-25.csv"
bq_table = "ecommerce_dataset.processed_sales"

# Read raw CSV file from GCS
df = spark.read.option("header", "true").csv(gcs_path)

# Clean the data
df_cleaned = df.dropDuplicates(["Order_ID"]).fillna({"Customer_ID": "Unknown"})

# Transformations
df_transformed = df_cleaned.withColumn("Total_Price", col("Price") * col("Quantity")) \
    .withColumn("Order_Date", to_date(col("Timestamp")))

# Save to BigQuery
df_transformed.write \
    .format("bigquery") \
    .option("temporaryGcsBucket", "ecommerce-temp-bucket") \
    .mode("overwrite") \
    .save(bq_table)

print("‚úÖ Data processing completed and saved to BigQuery!")
```

‚úÖ **This script does the following:**  
- **Reads raw data from GCS** (`sales_data_YYYY-MM-DD.csv`).  
- **Removes duplicates** based on `Order_ID`.  
- **Fills missing values** (`Customer_ID` ‚Üí `"Unknown"`).  
- **Calculates total price** (`Price * Quantity`).  
- **Converts timestamp into a clean date format** (`Order_Date`).  
- **Writes the transformed data to BigQuery**.

---

### **2Ô∏è‚É£ Automate Processing with Airflow DAG**
We don‚Äôt want to run this script manually. Instead, we automate it with **Airflow**.

üìå **Airflow DAG (`airflow/dags/process_data.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='process_sales_data',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

process_task = BashOperator(
    task_id='run_pyspark_processing',
    bash_command='python /opt/airflow/scripts/process_sales_data.py',
    dag=dag
)

process_task
```

‚úÖ **This Airflow DAG does the following:**  
- Runs the PySpark script **daily**.  
- Ensures that data is cleaned and **loaded into BigQuery** automatically.  

---

### **3Ô∏è‚É£ Further Transform Data in BigQuery Using dbt**
Once data is in **BigQuery**, we can perform additional transformations using **dbt**.

üìå **dbt Model (`dbt/models/clean_sales_data.sql`)**:
```sql
WITH transformed AS (
    SELECT
        Order_ID,
        Customer_ID,
        Product,
        Category,
        Total_Price,
        Order_Date
    FROM `ecommerce_dataset.processed_sales`
)
SELECT * FROM transformed
WHERE Total_Price > 0
```

‚úÖ **This dbt model does the following:**  
- Removes any **invalid transactions** (where `Total_Price <= 0`).  
- Ensures that only **clean data** is available for the dashboard.  

---

### **4Ô∏è‚É£ Automate dbt Execution with Airflow**
üìå **Airflow DAG (`airflow/dags/dbt_transformation.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='run_dbt_transformation',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dbt_task = BashOperator(
    task_id='run_dbt',
    bash_command='dbt run --profiles-dir /opt/airflow/dbt',
    dag=dag
)

dbt_task
```
‚úÖ **This DAG runs the dbt transformation daily after PySpark processing.**  

---

# **üéØ Summary of Step 2**
‚úî **PySpark reads raw data from GCS** ‚Üí Cleans and processes it.  
‚úî **Writes transformed data into BigQuery**.  
‚úî **dbt performs final transformations inside BigQuery**.  
‚úî **Airflow automates the entire pipeline**.  

---

# **üöÄ Next Steps**
‚úÖ Now that we have **cleaned and structured data in BigQuery**, we can move to **Step 3: Dashboard Creation using Power BI**. 

# **üì¶ Containerizing & Automating Step 2 with Dataproc & Docker**  

Since we want **full automation**, we will:  
‚úÖ **Containerize the entire data processing pipeline** using **Docker**.  
‚úÖ **Use Terraform** to provision **Google Dataproc** (a managed Spark service on GCP).  
‚úÖ **Use Airflow** to trigger Dataproc jobs automatically.  

---

## **üõ†Ô∏è Tech Stack for Automation**
| Technology  | Purpose  |
|-------------|----------|
| **Docker**  | Containerizes the PySpark job |
| **Google Dataproc** | Runs PySpark in a managed environment |
| **Terraform** | Creates Dataproc clusters automatically |
| **Apache Airflow** | Orchestrates Dataproc jobs |
| **Google Cloud Storage (GCS)** | Stores raw & intermediate data |
| **Google BigQuery (BQ)** | Stores final structured data |

---

## **üìÇ Step-by-Step Implementation**

### **1Ô∏è‚É£ Containerizing the PySpark Job**
We will run our **PySpark job inside a Docker container** so it can be deployed consistently.

üìå **Project Structure**:
```
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ process_sales_data.py
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ dataproc.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_data.py
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_sales_data.sql
```

---

üìå **Dockerfile for PySpark Job** (`docker/Dockerfile`):
```dockerfile
FROM gcr.io/dataproc-oss/pyspark:latest

WORKDIR /app

COPY process_sales_data.py .

CMD ["spark-submit", "process_sales_data.py"]
```
‚úÖ **This builds a container that runs PySpark on Dataproc.**

---

üìå **PySpark Job for Data Processing** (`docker/process_sales_data.py`):
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, to_date

# Initialize SparkSession
spark = SparkSession.builder.appName("SalesDataProcessing").getOrCreate()

# Define paths
gcs_input = "gs://ecommerce-raw-data/raw/sales_data.csv"
gcs_temp_bucket = "ecommerce-temp-bucket"
bq_table = "ecommerce_dataset.processed_sales"

# Read raw data from GCS
df = spark.read.option("header", "true").csv(gcs_input)

# Clean and transform data
df_cleaned = df.dropDuplicates(["Order_ID"]).fillna({"Customer_ID": "Unknown"})
df_transformed = df_cleaned.withColumn("Total_Price", col("Price") * col("Quantity")) \
    .withColumn("Order_Date", to_date(col("Timestamp")))

# Save to BigQuery
df_transformed.write \
    .format("bigquery") \
    .option("temporaryGcsBucket", gcs_temp_bucket) \
    .mode("overwrite") \
    .save(bq_table)

print("‚úÖ Data processing completed and saved to BigQuery!")
```
‚úÖ **This script reads data from GCS, processes it, and loads it into BigQuery.**

---

### **2Ô∏è‚É£ Automating Dataproc Cluster Creation with Terraform**
We use **Terraform** to automatically create a **Dataproc cluster**.

üìå **Terraform Configuration (`terraform/dataproc.tf`)**:
```hcl
provider "google" {
  project = "your-gcp-project-id"
  region  = "us-central1"
}

resource "google_dataproc_cluster" "dataproc-cluster" {
  name   = "dataproc-cluster"
  region = "us-central1"

  cluster_config {
    master_config {
      num_instances = 1
      machine_type  = "n1-standard-4"
    }

    worker_config {
      num_instances = 2
      machine_type  = "n1-standard-4"
    }

    software_config {
      image_version = "2.0-debian10"
      optional_components = ["JUPYTER"]
    }
  }
}
```
‚úÖ **This creates a Dataproc cluster with 1 master node and 2 worker nodes.**

üìå **Apply Terraform**:
```bash
cd terraform
terraform init
terraform apply -auto-approve
```
‚úÖ **This provisions the Dataproc cluster automatically.**

---

### **3Ô∏è‚É£ Running the PySpark Job on Dataproc**
Once the cluster is up, we **submit our PySpark job**.

üìå **Submit PySpark Job to Dataproc**:
```bash
gcloud dataproc jobs submit pyspark gs://ecommerce-raw-data/scripts/process_sales_data.py \
    --cluster=dataproc-cluster \
    --region=us-central1 \
    --async
```
‚úÖ **This runs our PySpark script on Dataproc.**

---

### **4Ô∏è‚É£ Automating the Entire Process with Airflow**
Instead of manually running the job, we use **Apache Airflow**.

üìå **Airflow DAG (`airflow/dags/process_data.py`)**:
```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='process_sales_data',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dataproc_job = {
    "reference": {"project_id": "your-gcp-project-id"},
    "placement": {"cluster_name": "dataproc-cluster"},
    "pyspark_job": {"main_python_file_uri": "gs://ecommerce-raw-data/scripts/process_sales_data.py"},
}

submit_job = DataprocSubmitJobOperator(
    task_id="submit_dataproc_job",
    job=dataproc_job,
    region="us-central1",
    project_id="your-gcp-project-id",
    dag=dag
)

submit_job
```
‚úÖ **This DAG triggers our Dataproc job every day.**

---

### **5Ô∏è‚É£ Automating dbt Transformations**
Once PySpark processing is done, we trigger **dbt** to run transformations inside BigQuery.

üìå **Airflow DAG for dbt (`airflow/dags/dbt_transformation.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='run_dbt_transformation',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dbt_task = BashOperator(
    task_id='run_dbt',
    bash_command='dbt run --profiles-dir /opt/airflow/dbt',
    dag=dag
)

dbt_task
```
‚úÖ **This DAG runs dbt models automatically after PySpark processing.**

---

## **üéØ Summary of Full Automation**
‚úî **Step 1: Terraform provisions the Dataproc cluster.**  
‚úî **Step 2: PySpark job is containerized with Docker & runs on Dataproc.**  
‚úî **Step 3: Airflow schedules & submits PySpark jobs to Dataproc.**  
‚úî **Step 4: Processed data is stored in BigQuery.**  
‚úî **Step 5: dbt runs additional transformations inside BigQuery.**  

---

# **üöÄ Next Steps**
‚úÖ Now that we have **fully automated data processing**, we can move to **Step 3: Dashboard Creation using Power BI**. 


# **üìä Step 3: Dashboard Creation with Power BI**  

Now that the **processed and transformed data** is stored in **Google BigQuery**, we will build a **Power BI dashboard** to visualize it.

---

## **üìÇ Overview of the Process**
‚úÖ **Connect Power BI to Google BigQuery**  
‚úÖ **Create Data Model & Transform Data if Needed**  
‚úÖ **Build Visualizations (Two Required Tiles)**  
‚úÖ **Publish & Share the Dashboard**  

---

## **1Ô∏è‚É£ Connecting Power BI to Google BigQuery**
Power BI **natively supports** Google BigQuery, but we need to configure it properly.

üìå **Steps to Connect**:  
1Ô∏è‚É£ Open Power BI and go to **"Home" ‚Üí "Get Data"**  
2Ô∏è‚É£ Search for **Google BigQuery** and select it  
3Ô∏è‚É£ Sign in with your **Google Cloud account**  
4Ô∏è‚É£ Select the **BigQuery dataset** where your transformed data is stored  
5Ô∏è‚É£ Load the data **directly** or use **Power Query to transform it further**  

‚úÖ **Example Query to Import Data**:  
```sql
SELECT 
    Order_ID, 
    Customer_ID, 
    Order_Date, 
    Total_Price, 
    Product_Category
FROM `ecommerce_dataset.processed_sales`
```
üìå **Tip:** Use **DirectQuery Mode** if you want real-time updates from BigQuery.

---

## **2Ô∏è‚É£ Data Modeling & Transformation in Power BI**
After importing data, we may need to clean and model it.

üìå **Key Tasks in Power BI's Power Query Editor**:
- **Rename columns** for better readability  
- **Convert data types** (e.g., ensure "Order_Date" is a Date type)  
- **Create calculated columns** (e.g., profit margins, customer segments)  
- **Add measures using DAX**  

‚úÖ **Example DAX Measure: Total Sales per Month**
```DAX
Total_Sales = SUM('processed_sales'[Total_Price])
```
‚úÖ **Example DAX Measure: Sales Growth Rate**
```DAX
Sales_Growth = 
    DIVIDE(
        [Total_Sales] - CALCULATE([Total_Sales], PREVIOUSMONTH('processed_sales'[Order_Date])),
        CALCULATE([Total_Sales], PREVIOUSMONTH('processed_sales'[Order_Date])),
        0
    )
```
---

## **3Ô∏è‚É£ Building the Dashboard**
We need **at least two tiles**, so let‚Äôs create **two key visualizations**.

### **üìå Tile 1: Sales Distribution by Product Category**
‚úÖ **Chart Type:** Bar Chart  
‚úÖ **Purpose:** Shows which product categories generate the most sales.  
üìå **Steps**:  
1Ô∏è‚É£ Drag **Product_Category** to the X-axis  
2Ô∏è‚É£ Drag **Total_Sales** to the Y-axis  
3Ô∏è‚É£ Apply sorting by **descending order**  
4Ô∏è‚É£ Format with a title and labels  

üîç **Example Insight:**  
*‚ÄúElectronics contribute to 40% of total sales, making it the highest-selling category.‚Äù*

---

### **üìå Tile 2: Monthly Sales Trend Over Time**
‚úÖ **Chart Type:** Line Chart  
‚úÖ **Purpose:** Tracks sales growth over time.  
üìå **Steps**:  
1Ô∏è‚É£ Drag **Order_Date** to the X-axis  
2Ô∏è‚É£ Drag **Total_Sales** to the Y-axis  
3Ô∏è‚É£ Format the X-axis to **"Month-Year"**  
4Ô∏è‚É£ Add a **trendline** for better insights  

üîç **Example Insight:**  
*‚ÄúSales peak in November and December due to Black Friday and holiday shopping.‚Äù*

---

## **4Ô∏è‚É£ Publishing & Sharing the Dashboard**
After creating the dashboard, we need to **share it** with stakeholders.

üìå **Steps to Publish**:  
1Ô∏è‚É£ Click **"Publish"** in Power BI  
2Ô∏è‚É£ Choose **Power BI Service (Online)**  
3Ô∏è‚É£ Create a **workspace** and upload your report  
4Ô∏è‚É£ Set up **data refresh schedule** (if using DirectQuery)  
5Ô∏è‚É£ Share access with **team members or stakeholders**  

‚úÖ **Automating Refresh (Optional)**:  
- If using **Import Mode**, schedule daily/weekly refresh  
- If using **DirectQuery**, the data updates automatically  

---

## **üéØ Summary of Step 3**
‚úî **Connected Power BI to BigQuery**  
‚úî **Transformed data & created measures in Power BI**  
‚úî **Built two visualizations (Sales by Category & Monthly Sales Trend)**  
‚úî **Published and shared the dashboard**  

---

# **üöÄ Next Steps**
‚úÖ Now that the **dashboard is ready**, do you want to explore **automating Power BI refreshes** or **optimizing the data model further**? Let me know how you‚Äôd like to proceed!

# **üöÄ Automating Power BI Refresh & Optimizing Data Model**  

Now that the **dashboard is built**, we‚Äôll focus on:  
‚úÖ **Automating Data Refresh** ‚Äì Ensuring the Power BI report always has the latest data.  
‚úÖ **Optimizing Data Model** ‚Äì Improving performance and reducing query costs.

---

## **üõ†Ô∏è 1Ô∏è‚É£ Automating Power BI Data Refresh**  

### **Option 1: Scheduled Refresh in Power BI Service (for Import Mode)**
If we **imported data** from BigQuery, we need to schedule periodic refreshes.

üìå **Steps to Enable Scheduled Refresh:**  
1Ô∏è‚É£ Go to **Power BI Service** ‚Üí Select your **Published Report**  
2Ô∏è‚É£ Click on **Dataset Settings**  
3Ô∏è‚É£ Under **Gateway & Cloud Connection**, ensure the Google BigQuery connection is set up  
4Ô∏è‚É£ Enable **"Scheduled Refresh"**  
5Ô∏è‚É£ Set the refresh frequency (**Hourly, Daily, Weekly**)  

‚úÖ **Best Practice:** Set refresh to **every 6-12 hours** to balance performance & cost.

---

### **Option 2: Using DirectQuery (for Real-time Updates)**
If we **used DirectQuery**, data updates automatically without needing scheduled refreshes.

‚úÖ **Pros:**  
‚úî No need for periodic refreshes.  
‚úî Always fetches live data from BigQuery.  

‚úÖ **Cons:**  
‚ùå Slower performance since queries run in real-time.  
‚ùå BigQuery costs may increase due to frequent queries.  

üìå **When to Use DirectQuery?**  
Use DirectQuery if **data needs to be updated in real-time** (e.g., stock market dashboards, live sales reports).

---

### **Option 3: Automating Refresh with Power Automate**  
For **more control over refresh timing**, we can use **Power Automate**.

üìå **Steps to Set Up Power Automate for Refresh**  
1Ô∏è‚É£ Go to **Power Automate** ‚Üí Click **New Flow**  
2Ô∏è‚É£ Choose **Scheduled Cloud Flow**  
3Ô∏è‚É£ Select **Power BI ‚Üí Refresh Dataset** action  
4Ô∏è‚É£ Choose the **Power BI workspace & dataset**  
5Ô∏è‚É£ Set **trigger schedule** (e.g., every 4 hours)  
6Ô∏è‚É£ Click **Save & Activate**  

‚úÖ **Benefit**: More flexibility in scheduling refreshes (e.g., trigger refresh **only when new data is added**).

---

## **üìä 2Ô∏è‚É£ Optimizing Power BI Data Model**  
To **improve performance and reduce costs**, we‚Äôll optimize **data structure & queries**.

### **üìå Best Practices for Data Modeling**
‚úî **Use Aggregations** ‚Äì Instead of detailed raw data, pre-aggregate totals in BigQuery.  
‚úî **Reduce Column Count** ‚Äì Only import the **necessary fields**.  
‚úî **Optimize Relationships** ‚Äì Use **star schema** instead of direct table joins.  
‚úî **Partition & Cluster BigQuery Tables** ‚Äì Improves query speed.  

‚úÖ **Example: Optimize a Query in BigQuery**  
Before (Inefficient Query üõë ‚Äì Scans Entire Table):
```sql
SELECT * FROM `ecommerce_dataset.sales_data`
WHERE Order_Date BETWEEN '2024-01-01' AND '2024-03-31'
```
After (Optimized Query ‚úÖ ‚Äì Uses Partitioning & Clustering):
```sql
SELECT Order_ID, Customer_ID, Total_Price
FROM `ecommerce_dataset.sales_data`
WHERE Order_Date >= DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH)
```
üìå **Why?**  
- Uses **partitioning** ‚Üí Queries only **recent data**, reducing scan cost.  
- Limits columns ‚Üí Only retrieves what Power BI needs.  

---

### **üìå Reducing Query Load on BigQuery**
Since Power BI can query BigQuery **frequently**, we need to **optimize performance**.

‚úÖ **Strategies**:
‚úî **Use Import Mode for Large Datasets** (Avoids running queries every time a user loads the dashboard).  
‚úî **Cache Data in Power BI** ‚Äì Reduces calls to BigQuery.  
‚úî **Create Materialized Views in BigQuery** ‚Äì Precompute reports for faster performance.  

---

## **üéØ Summary & Next Steps**
‚úî **Enabled Automated Data Refresh** (Scheduled Refresh, DirectQuery, Power Automate).  
‚úî **Optimized Power BI Data Model** (Better queries, fewer columns, aggregations).  
‚úî **Reduced BigQuery Costs** (Partitioning, Materialized Views, Import Mode).  

üöÄ **Next Step:**  
Would you like to set up **CI/CD automation for deploying Power BI reports** or optimize **BigQuery cost further**? Let me know how you want to proceed! üòä








