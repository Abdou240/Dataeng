Hereâ€™s a simplified breakdown of your end-to-end data pipeline:  

### **1. Data Ingestion (Batch Processing)**  
- **Google Cloud Storage (GCS)** â†’ Stores raw data.  
- **Terraform** â†’ Provisions infrastructure (GCS, BigQuery, Airflow).  
- **Docker** â†’ Containerizes the pipeline.  
- **Apache Airflow** â†’ Orchestrates batch processing.  

### **2. Data Processing**  
- **PySpark** â†’ Cleans and processes raw data.  
- **Airflow DAG** â†’ Moves processed data to BigQuery.  

### **3. Data Warehousing**  
- **Google BigQuery** â†’ Stores structured, partitioned, and clustered data.  

### **4. Data Transformation**  
- **dbt (Data Build Tool)** â†’ Applies SQL-based transformations on BigQuery.  

### **5. Data Visualization**  
- **Power BI** â†’ Connects to BigQuery and creates dashboards.  

### **6. Infrastructure as Code**  
- **Terraform** â†’ Deploys and manages the infrastructure.  

Hereâ€™s a **detailed explanation** of each step in your **end-to-end data pipeline** using GCP and modern data tools:

---

## **1. Data Ingestion (Batch Processing)**
This is the first step where raw data is collected and stored before processing.

- **Google Cloud Storage (GCS)**:  
  - Acts as a **data lake** where raw files (CSV, JSON, Parquet, etc.) are stored.  
  - You can upload data manually or automate it via scripts/API.  

- **Terraform (Infrastructure as Code - IaC)**:  
  - Automates the provisioning of GCP resources like **GCS, BigQuery, and Airflow**.  
  - Ensures infrastructure is reproducible and version-controlled.  

- **Docker (Containerization)**:  
  - Packages applications and dependencies into **containers** for consistency across environments.  
  - Helps deploy Airflow, PySpark jobs, and dbt in an isolated environment.  

- **Apache Airflow (Workflow Orchestration)**:  
  - Manages and schedules batch jobs using **DAGs (Directed Acyclic Graphs)**.  
  - Defines steps like:  
    1. Extract data from **GCS**.  
    2. Process it using **PySpark**.  
    3. Load it into **BigQuery**.  

---

## **2. Data Processing**
After ingestion, data needs cleaning and transformation before storage.

- **PySpark (Batch Processing Framework)**:  
  - Runs distributed processing for large datasets.  
  - Performs data cleaning, filtering, aggregation, and transformations.  
  - Converts raw data into structured format before loading into BigQuery.  

- **Airflow DAGs (Task Automation)**:  
  - Automates PySpark processing.  
  - Moves processed data from GCS to BigQuery in an **ETL pipeline**.  

---

## **3. Data Warehousing**
Once processed, data is stored in a structured format for easy querying.

- **Google BigQuery (Cloud Data Warehouse)**:  
  - Stores **processed, structured data**.  
  - Supports **partitioning (by date)** and **clustering (by category)** to improve performance.  
  - Handles **large-scale analytics** with SQL queries.  

---

## **4. Data Transformation**
After storing the data, transformations are applied to prepare it for analytics.

- **dbt (Data Build Tool)**:  
  - Defines transformations using **SQL models**.  
  - Creates **views and tables** in BigQuery for analysis.  
  - Automates **incremental updates** to optimize queries.  

Example:  
- Raw table: **user_clicks_raw**  
- Transformed table: **daily_user_activity_summary**  

---

## **5. Data Visualization**
Now that the data is structured and transformed, itâ€™s ready for reporting.

- **Power BI (Business Intelligence Tool)**:  
  - Connects to **BigQuery** for real-time dashboards.  
  - Creates **two types of visualizations**:  
    1. **Categorical Distribution Graph** (e.g., Top 10 products by sales).  
    2. **Time Series Graph** (e.g., Sales growth over time).  
  - Ensures interactive and easy-to-read dashboards.  

---

## **6. Infrastructure as Code (Automation & Deployment)**
To keep the pipeline scalable and reproducible, infrastructure is automated.

- **Terraform**:  
  - Deploys **GCS, BigQuery, Airflow, dbt, and other resources** with a single script.  
  - Ensures everything is **version-controlled and repeatable**.  

---

## **Final Workflow Summary**
1. **Raw Data** â†’ Stored in **GCS**.  
2. **Airflow DAG** â†’ Triggers **PySpark** processing.  
3. **PySpark** â†’ Cleans and loads data into **BigQuery**.  
4. **dbt** â†’ Transforms data for analytics.  
5. **Power BI** â†’ Visualizes insights for decision-making.  

---

This **end-to-end pipeline** ensures **automated, scalable, and efficient data processing** using GCP tools.


### **Step 1: Data Ingestion (Batch Processing) - Detailed Breakdown**  
The first step is to **ingest raw data** and store it in a **data lake** for further processing. This step ensures that data is collected, stored securely, and is available for processing in a structured way.

---

## **ðŸ“Œ Objective of This Step**  
We need to collect raw data from a source, store it in **Google Cloud Storage (GCS)**, and automate infrastructure deployment using **Terraform**.

---

## **ðŸ“‚ Example Use Case: E-commerce Sales Data**  
Let's assume we are working with an **e-commerce dataset** that contains **customer transactions**. The data is generated by the online store daily in CSV format.  
   
**Example raw data file (`sales_data_2025-03-25.csv`)**:  
| Order_ID | Customer_ID | Product | Category | Price | Quantity | Timestamp           |  
|----------|------------|---------|----------|-------|----------|---------------------|  
| 1001     | 500        | Laptop  | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  
| 1002     | 501        | Phone   | Electronics | 800   | 2        | 2025-03-25 09:00:00 |  

---

## **ðŸ› ï¸ Technologies Used & Why?**  

| Technology  | Purpose  | How We Use It?  |  
|-------------|----------|----------------|  
| **Google Cloud Storage (GCS)** | Stores raw data | Upload CSV files to a **dedicated bucket** for processing |  
| **Terraform (IaC)** | Automates cloud resource creation | Creates the GCS bucket and IAM roles programmatically |  
| **Docker** | Ensures consistent environment | Runs Airflow in a **containerized setup** |  
| **Apache Airflow** | Orchestrates the pipeline | Automates file ingestion from a local system to GCS |  
| **Bash (Shell Scripting)** | Uploads files programmatically | Automates file transfers to GCS |  

---

## **ðŸ› ï¸ Step-by-Step Implementation**

### **1ï¸âƒ£ Create Google Cloud Storage (GCS) Bucket**  
Before uploading files, we need a **GCS bucket** to store raw data.

- **Terraform Script to Create GCS Bucket (`gcs.tf`)**:
```hcl
resource "google_storage_bucket" "raw_data_bucket" {
  name     = "ecommerce-raw-data"
  location = "US"
  storage_class = "STANDARD"
}
```
âœ… **What this does?**  
- Creates a **GCS bucket** named `"ecommerce-raw-data"`.  
- Sets the **storage class** to `"STANDARD"` for cost efficiency.  

---

### **2ï¸âƒ£ Upload Data to GCS (Using Bash Script & Airflow)**  
Once the bucket is created, we need to **upload raw CSV files**.

- **Bash Script to Upload Data (`upload_data.sh`)**:
```bash
#!/bin/bash
BUCKET_NAME="ecommerce-raw-data"
FILE_PATH="data/sales_data_2025-03-25.csv"

# Upload file to GCS
gsutil cp $FILE_PATH gs://$BUCKET_NAME/raw/
echo "File uploaded successfully to GCS: $BUCKET_NAME"
```
âœ… **What this does?**  
- Uses **`gsutil cp`** to copy a local file (`sales_data_2025-03-25.csv`) to GCS.  
- Stores the file inside a `"raw/"` directory in GCS.  

---

### **3ï¸âƒ£ Automate File Upload with Apache Airflow**  
We donâ€™t want to manually run the script daily. Instead, we use **Airflow** to automate the process.

- **Airflow DAG to Upload Data (`upload_to_gcs.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

# Define default arguments
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 25),
    'retries': 1
}

# Define the DAG
dag = DAG(
    dag_id='upload_sales_data_to_gcs',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

# Define the task
upload_task = BashOperator(
    task_id='upload_csv_to_gcs',
    bash_command='bash /opt/airflow/scripts/upload_data.sh',
    dag=dag
)

upload_task
```
âœ… **What this does?**  
- **Schedules** the upload task to run **daily**.  
- **Triggers the Bash script** (`upload_data.sh`) to copy files to GCS.  

---

### **4ï¸âƒ£ Verify Data in GCS**  
After execution, check if the file is uploaded:  
```bash
gsutil ls gs://ecommerce-raw-data/raw/
```
ðŸ”¹ Expected Output:  
```
gs://ecommerce-raw-data/raw/sales_data_2025-03-25.csv
```
---

## **ðŸŽ¯ Summary of Step 1**
âœ”ï¸ **Created a GCS bucket** for raw data storage using **Terraform**.  
âœ”ï¸ **Wrote a Bash script** to upload CSV files to **GCS**.  
âœ”ï¸ **Automated the upload process** using **Apache Airflow DAGs**.  
âœ”ï¸ **Ensured repeatability** using **Docker** and **Terraform**.  

---

## **Next Steps**
âœ… Now that the raw data is in GCS, the next step is **processing it using PySpark** before loading it into **BigQuery**. Let me know if you want a similar detailed breakdown for the next step!

If we want to **fully automate Step 1**, including **Terraform deployment, data upload, and orchestration**, we need to containerize the entire process using **Docker**. Hereâ€™s how we can achieve that:

---

# **ðŸ“Œ High-Level Overview**
We will create a **Dockerized environment** that automates:
1. **Infrastructure deployment** using Terraform.  
2. **File upload to GCS** using a Bash script.  
3. **Orchestration** using Apache Airflow to trigger the upload task daily.  

---

## **ðŸ“‚ Folder Structure**
```
data-pipeline/
â”‚â”€â”€ airflow/                      # Airflow-related files
â”‚   â”œâ”€â”€ dags/                     # DAGs for automation
â”‚   â”‚   â”œâ”€â”€ upload_to_gcs.py      # DAG to automate file upload
â”‚   â”œâ”€â”€ Dockerfile                 # Airflow Docker setup
â”‚â”€â”€ terraform/                     # Terraform files
â”‚   â”œâ”€â”€ main.tf                    # Terraform script to create resources
â”‚   â”œâ”€â”€ variables.tf                # Terraform variables
â”‚   â”œâ”€â”€ terraform.sh                # Shell script to apply Terraform
â”‚â”€â”€ scripts/                        # Helper scripts
â”‚   â”œâ”€â”€ upload_data.sh              # Script to upload data to GCS
â”‚â”€â”€ docker-compose.yml               # Docker Compose to run everything
â”‚â”€â”€ .env                             # Environment variables
```

---

# **ðŸ› ï¸ Step-by-Step Implementation**

### **1ï¸âƒ£ Write the Terraform Configuration**
Create the **Terraform script** (`terraform/main.tf`) to set up GCS:

```hcl
provider "google" {
  project = var.project_id
  region  = var.region
}

resource "google_storage_bucket" "raw_data_bucket" {
  name     = "ecommerce-raw-data"
  location = var.region
  storage_class = "STANDARD"
}
```
âœ… **This creates a GCS bucket in GCP.**  

---

### **2ï¸âƒ£ Automate Terraform Deployment**
Create a **shell script** (`terraform/terraform.sh`) to automate Terraform execution:

```bash
#!/bin/bash
echo "Initializing Terraform..."
terraform init

echo "Applying Terraform..."
terraform apply -auto-approve

echo "Terraform deployment completed."
```
âœ… **This script automatically initializes and applies Terraform.**  

---

### **3ï¸âƒ£ Automate Data Upload to GCS**
Create a **Bash script** (`scripts/upload_data.sh`) to upload files:

```bash
#!/bin/bash
BUCKET_NAME="ecommerce-raw-data"
FILE_PATH="/opt/airflow/data/sales_data_$(date +%Y-%m-%d).csv"

echo "Uploading data to GCS..."
gsutil cp $FILE_PATH gs://$BUCKET_NAME/raw/

echo "File successfully uploaded!"
```
âœ… **This uploads a CSV file with the current date to GCS.**  

---

### **4ï¸âƒ£ Create Airflow DAG for Automation**
Create an **Airflow DAG** (`airflow/dags/upload_to_gcs.py`):

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 25),
    'retries': 1
}

dag = DAG(
    dag_id='upload_sales_data_to_gcs',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

upload_task = BashOperator(
    task_id='upload_csv_to_gcs',
    bash_command='bash /opt/airflow/scripts/upload_data.sh',
    dag=dag
)

upload_task
```
âœ… **This DAG schedules the upload task daily.**  

---

### **5ï¸âƒ£ Create Dockerfile for Airflow**
Create an **Airflow Dockerfile** (`airflow/Dockerfile`):

```dockerfile
FROM apache/airflow:2.5.1

USER root

# Install Google Cloud CLI for GCS operations
RUN apt-get update && apt-get install -y google-cloud-sdk

# Set working directory
WORKDIR /opt/airflow

# Copy scripts and DAGs
COPY dags /opt/airflow/dags
COPY scripts /opt/airflow/scripts
COPY terraform /opt/airflow/terraform

# Change permissions
RUN chmod +x /opt/airflow/scripts/upload_data.sh
RUN chmod +x /opt/airflow/terraform/terraform.sh

# Install dependencies
RUN pip install apache-airflow-providers-google

# Start Airflow
CMD ["airflow", "scheduler"]
```
âœ… **This image sets up Airflow with Google Cloud SDK and required scripts.**  

---

### **6ï¸âƒ£ Define Docker Compose to Run Everything**
Create a **Docker Compose file** (`docker-compose.yml`) to run all components:

```yaml
version: '3.8'
services:
  terraform:
    image: hashicorp/terraform:latest
    container_name: terraform
    working_dir: /opt/airflow/terraform
    volumes:
      - ./terraform:/opt/airflow/terraform
    entrypoint: ["/bin/sh", "-c", "./terraform.sh"]

  airflow:
    build: ./airflow
    container_name: airflow
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/gcp-credentials.json
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./terraform:/opt/airflow/terraform
    ports:
      - "8080:8080"
    depends_on:
      - terraform
```
âœ… **This setup ensures:**  
- **Terraform runs first** to create infrastructure.  
- **Airflow starts automatically** and schedules the upload task.  

---

### **7ï¸âƒ£ Run Everything**
Now, simply run the following commands:

```bash
# Start the whole pipeline
docker-compose up --build
```
---

## **âœ… Final Workflow**
1ï¸âƒ£ **Terraform runs inside Docker** â†’ Creates GCS bucket.  
2ï¸âƒ£ **Airflow container starts** and schedules data upload.  
3ï¸âƒ£ **Airflow executes the DAG** â†’ Calls `upload_data.sh`.  
4ï¸âƒ£ **Bash script uploads CSV files to GCS** automatically.  

---

## **ðŸš€ Advantages of This Setup**
âœ” **Fully automated** â†’ No manual steps needed.  
âœ” **Containerized** â†’ Works consistently across machines.  
âœ” **Infrastructure as Code (IaC)** â†’ Easily reproducible.  
âœ” **Orchestrated** â†’ Airflow ensures proper scheduling.  

---

This **Dockerized data ingestion process** ensures a **scalable, repeatable, and automated pipeline** for storing raw data in GCS. 

# **ðŸ“Œ Step 2: Data Processing (Batch Processing with PySpark & dbt)**  

Now that raw data is stored in **Google Cloud Storage (GCS)**, we need to **process and clean** it before loading it into **Google BigQuery (BQ)**. This ensures that data is structured, optimized, and ready for analytics.

---

## **ðŸŽ¯ Objective of Step 2**  
1. **Extract** raw data from GCS.  
2. **Clean & transform** it using **PySpark**.  
3. **Load the processed data** into **Google BigQuery (BQ)**.  
4. **Use dbt (Data Build Tool)** for further transformations inside BigQuery.  

---

## **ðŸ“‚ Example Use Case: E-commerce Sales Data**  
The raw data (`sales_data_YYYY-MM-DD.csv`) from Step 1 has the following issues:  
âœ… **Missing values** (e.g., some customers don't have a Customer_ID).  
âœ… **Duplicate records** (e.g., repeated transactions).  
âœ… **Inconsistent formatting** (e.g., date columns have mixed formats).  
âœ… **Unnecessary columns** that wonâ€™t be used in analytics.  

**Example Raw Data (Before Processing)**:
| Order_ID | Customer_ID | Product  | Category    | Price | Quantity | Timestamp           |  
|----------|------------|----------|------------|-------|----------|---------------------|  
| 1001     | 500        | Laptop   | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  
| 1002     | NULL       | Phone    | Electronics | 800   | 2        | 2025-03-25 09:00:00 |  
| 1003     | 500        | Laptop   | Electronics | 1200  | 1        | 2025-03-25 08:30:00 |  

**Example Processed Data (After PySpark Cleaning & Transformation)**:
| Order_ID | Customer_ID | Product  | Category    | Total_Price | Order_Date  |  
|----------|------------|----------|------------|------------|------------|  
| 1001     | 500        | Laptop   | Electronics | 1200       | 2025-03-25 |  
| 1002     | Unknown    | Phone    | Electronics | 1600       | 2025-03-25 |  

---

## **ðŸ› ï¸ Technologies Used & Why?**

| Technology  | Purpose  | How We Use It?  |  
|-------------|----------|----------------|  
| **PySpark** | Big data processing | Reads raw data from GCS, cleans it, and transforms it |  
| **Google BigQuery (BQ)** | Cloud data warehouse | Stores the processed data for analytics |  
| **dbt (Data Build Tool)** | Data transformation inside BQ | Performs final modeling and aggregations |  
| **Apache Airflow** | Orchestration | Automates the PySpark job and dbt execution |  

---

# **ðŸ› ï¸ Step-by-Step Implementation**

### **1ï¸âƒ£ Read Raw Data from GCS Using PySpark**  
We'll create a PySpark job to read data from GCS.

ðŸ“Œ **PySpark Script (`scripts/process_sales_data.py`)**:
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, sum, to_date

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("SalesDataProcessing") \
    .getOrCreate()

# Define GCS source and BQ destination
gcs_path = "gs://ecommerce-raw-data/raw/sales_data_2025-03-25.csv"
bq_table = "ecommerce_dataset.processed_sales"

# Read raw CSV file from GCS
df = spark.read.option("header", "true").csv(gcs_path)

# Clean the data
df_cleaned = df.dropDuplicates(["Order_ID"]).fillna({"Customer_ID": "Unknown"})

# Transformations
df_transformed = df_cleaned.withColumn("Total_Price", col("Price") * col("Quantity")) \
    .withColumn("Order_Date", to_date(col("Timestamp")))

# Save to BigQuery
df_transformed.write \
    .format("bigquery") \
    .option("temporaryGcsBucket", "ecommerce-temp-bucket") \
    .mode("overwrite") \
    .save(bq_table)

print("âœ… Data processing completed and saved to BigQuery!")
```

âœ… **This script does the following:**  
- **Reads raw data from GCS** (`sales_data_YYYY-MM-DD.csv`).  
- **Removes duplicates** based on `Order_ID`.  
- **Fills missing values** (`Customer_ID` â†’ `"Unknown"`).  
- **Calculates total price** (`Price * Quantity`).  
- **Converts timestamp into a clean date format** (`Order_Date`).  
- **Writes the transformed data to BigQuery**.

---

### **2ï¸âƒ£ Automate Processing with Airflow DAG**
We donâ€™t want to run this script manually. Instead, we automate it with **Airflow**.

ðŸ“Œ **Airflow DAG (`airflow/dags/process_data.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='process_sales_data',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

process_task = BashOperator(
    task_id='run_pyspark_processing',
    bash_command='python /opt/airflow/scripts/process_sales_data.py',
    dag=dag
)

process_task
```

âœ… **This Airflow DAG does the following:**  
- Runs the PySpark script **daily**.  
- Ensures that data is cleaned and **loaded into BigQuery** automatically.  

---

### **3ï¸âƒ£ Further Transform Data in BigQuery Using dbt**
Once data is in **BigQuery**, we can perform additional transformations using **dbt**.

ðŸ“Œ **dbt Model (`dbt/models/clean_sales_data.sql`)**:
```sql
WITH transformed AS (
    SELECT
        Order_ID,
        Customer_ID,
        Product,
        Category,
        Total_Price,
        Order_Date
    FROM `ecommerce_dataset.processed_sales`
)
SELECT * FROM transformed
WHERE Total_Price > 0
```

âœ… **This dbt model does the following:**  
- Removes any **invalid transactions** (where `Total_Price <= 0`).  
- Ensures that only **clean data** is available for the dashboard.  

---

### **4ï¸âƒ£ Automate dbt Execution with Airflow**
ðŸ“Œ **Airflow DAG (`airflow/dags/dbt_transformation.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='run_dbt_transformation',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dbt_task = BashOperator(
    task_id='run_dbt',
    bash_command='dbt run --profiles-dir /opt/airflow/dbt',
    dag=dag
)

dbt_task
```
âœ… **This DAG runs the dbt transformation daily after PySpark processing.**  

---

# **ðŸŽ¯ Summary of Step 2**
âœ” **PySpark reads raw data from GCS** â†’ Cleans and processes it.  
âœ” **Writes transformed data into BigQuery**.  
âœ” **dbt performs final transformations inside BigQuery**.  
âœ” **Airflow automates the entire pipeline**.  

---

# **ðŸš€ Next Steps**
âœ… Now that we have **cleaned and structured data in BigQuery**, we can move to **Step 3: Dashboard Creation using Power BI**. 

# **ðŸ“¦ Containerizing & Automating Step 2 with Dataproc & Docker**  

Since we want **full automation**, we will:  
âœ… **Containerize the entire data processing pipeline** using **Docker**.  
âœ… **Use Terraform** to provision **Google Dataproc** (a managed Spark service on GCP).  
âœ… **Use Airflow** to trigger Dataproc jobs automatically.  

---

## **ðŸ› ï¸ Tech Stack for Automation**
| Technology  | Purpose  |
|-------------|----------|
| **Docker**  | Containerizes the PySpark job |
| **Google Dataproc** | Runs PySpark in a managed environment |
| **Terraform** | Creates Dataproc clusters automatically |
| **Apache Airflow** | Orchestrates Dataproc jobs |
| **Google Cloud Storage (GCS)** | Stores raw & intermediate data |
| **Google BigQuery (BQ)** | Stores final structured data |

---

## **ðŸ“‚ Step-by-Step Implementation**

### **1ï¸âƒ£ Containerizing the PySpark Job**
We will run our **PySpark job inside a Docker container** so it can be deployed consistently.

ðŸ“Œ **Project Structure**:
```
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ process_sales_data.py
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ dataproc.tf
â”‚   â”œâ”€â”€ variables.tf
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ process_data.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ clean_sales_data.sql
```

---

ðŸ“Œ **Dockerfile for PySpark Job** (`docker/Dockerfile`):
```dockerfile
FROM gcr.io/dataproc-oss/pyspark:latest

WORKDIR /app

COPY process_sales_data.py .

CMD ["spark-submit", "process_sales_data.py"]
```
âœ… **This builds a container that runs PySpark on Dataproc.**

---

ðŸ“Œ **PySpark Job for Data Processing** (`docker/process_sales_data.py`):
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, to_date

# Initialize SparkSession
spark = SparkSession.builder.appName("SalesDataProcessing").getOrCreate()

# Define paths
gcs_input = "gs://ecommerce-raw-data/raw/sales_data.csv"
gcs_temp_bucket = "ecommerce-temp-bucket"
bq_table = "ecommerce_dataset.processed_sales"

# Read raw data from GCS
df = spark.read.option("header", "true").csv(gcs_input)

# Clean and transform data
df_cleaned = df.dropDuplicates(["Order_ID"]).fillna({"Customer_ID": "Unknown"})
df_transformed = df_cleaned.withColumn("Total_Price", col("Price") * col("Quantity")) \
    .withColumn("Order_Date", to_date(col("Timestamp")))

# Save to BigQuery
df_transformed.write \
    .format("bigquery") \
    .option("temporaryGcsBucket", gcs_temp_bucket) \
    .mode("overwrite") \
    .save(bq_table)

print("âœ… Data processing completed and saved to BigQuery!")
```
âœ… **This script reads data from GCS, processes it, and loads it into BigQuery.**

---

### **2ï¸âƒ£ Automating Dataproc Cluster Creation with Terraform**
We use **Terraform** to automatically create a **Dataproc cluster**.

ðŸ“Œ **Terraform Configuration (`terraform/dataproc.tf`)**:
```hcl
provider "google" {
  project = "your-gcp-project-id"
  region  = "us-central1"
}

resource "google_dataproc_cluster" "dataproc-cluster" {
  name   = "dataproc-cluster"
  region = "us-central1"

  cluster_config {
    master_config {
      num_instances = 1
      machine_type  = "n1-standard-4"
    }

    worker_config {
      num_instances = 2
      machine_type  = "n1-standard-4"
    }

    software_config {
      image_version = "2.0-debian10"
      optional_components = ["JUPYTER"]
    }
  }
}
```
âœ… **This creates a Dataproc cluster with 1 master node and 2 worker nodes.**

ðŸ“Œ **Apply Terraform**:
```bash
cd terraform
terraform init
terraform apply -auto-approve
```
âœ… **This provisions the Dataproc cluster automatically.**

---

### **3ï¸âƒ£ Running the PySpark Job on Dataproc**
Once the cluster is up, we **submit our PySpark job**.

ðŸ“Œ **Submit PySpark Job to Dataproc**:
```bash
gcloud dataproc jobs submit pyspark gs://ecommerce-raw-data/scripts/process_sales_data.py \
    --cluster=dataproc-cluster \
    --region=us-central1 \
    --async
```
âœ… **This runs our PySpark script on Dataproc.**

---

### **4ï¸âƒ£ Automating the Entire Process with Airflow**
Instead of manually running the job, we use **Apache Airflow**.

ðŸ“Œ **Airflow DAG (`airflow/dags/process_data.py`)**:
```python
from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='process_sales_data',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dataproc_job = {
    "reference": {"project_id": "your-gcp-project-id"},
    "placement": {"cluster_name": "dataproc-cluster"},
    "pyspark_job": {"main_python_file_uri": "gs://ecommerce-raw-data/scripts/process_sales_data.py"},
}

submit_job = DataprocSubmitJobOperator(
    task_id="submit_dataproc_job",
    job=dataproc_job,
    region="us-central1",
    project_id="your-gcp-project-id",
    dag=dag
)

submit_job
```
âœ… **This DAG triggers our Dataproc job every day.**

---

### **5ï¸âƒ£ Automating dbt Transformations**
Once PySpark processing is done, we trigger **dbt** to run transformations inside BigQuery.

ðŸ“Œ **Airflow DAG for dbt (`airflow/dags/dbt_transformation.py`)**:
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 3, 26),
    'retries': 1
}

dag = DAG(
    dag_id='run_dbt_transformation',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

dbt_task = BashOperator(
    task_id='run_dbt',
    bash_command='dbt run --profiles-dir /opt/airflow/dbt',
    dag=dag
)

dbt_task
```
âœ… **This DAG runs dbt models automatically after PySpark processing.**

---

## **ðŸŽ¯ Summary of Full Automation**
âœ” **Step 1: Terraform provisions the Dataproc cluster.**  
âœ” **Step 2: PySpark job is containerized with Docker & runs on Dataproc.**  
âœ” **Step 3: Airflow schedules & submits PySpark jobs to Dataproc.**  
âœ” **Step 4: Processed data is stored in BigQuery.**  
âœ” **Step 5: dbt runs additional transformations inside BigQuery.**  

---

# **ðŸš€ Next Steps**
âœ… Now that we have **fully automated data processing**, we can move to **Step 3: Dashboard Creation using Power BI**. 


# **ðŸ“Š Step 3: Dashboard Creation with Power BI**  

Now that the **processed and transformed data** is stored in **Google BigQuery**, we will build a **Power BI dashboard** to visualize it.

---

## **ðŸ“‚ Overview of the Process**
âœ… **Connect Power BI to Google BigQuery**  
âœ… **Create Data Model & Transform Data if Needed**  
âœ… **Build Visualizations (Two Required Tiles)**  
âœ… **Publish & Share the Dashboard**  

---

## **1ï¸âƒ£ Connecting Power BI to Google BigQuery**
Power BI **natively supports** Google BigQuery, but we need to configure it properly.

ðŸ“Œ **Steps to Connect**:  
1ï¸âƒ£ Open Power BI and go to **"Home" â†’ "Get Data"**  
2ï¸âƒ£ Search for **Google BigQuery** and select it  
3ï¸âƒ£ Sign in with your **Google Cloud account**  
4ï¸âƒ£ Select the **BigQuery dataset** where your transformed data is stored  
5ï¸âƒ£ Load the data **directly** or use **Power Query to transform it further**  

âœ… **Example Query to Import Data**:  
```sql
SELECT 
    Order_ID, 
    Customer_ID, 
    Order_Date, 
    Total_Price, 
    Product_Category
FROM `ecommerce_dataset.processed_sales`
```
ðŸ“Œ **Tip:** Use **DirectQuery Mode** if you want real-time updates from BigQuery.

---

## **2ï¸âƒ£ Data Modeling & Transformation in Power BI**
After importing data, we may need to clean and model it.

ðŸ“Œ **Key Tasks in Power BI's Power Query Editor**:
- **Rename columns** for better readability  
- **Convert data types** (e.g., ensure "Order_Date" is a Date type)  
- **Create calculated columns** (e.g., profit margins, customer segments)  
- **Add measures using DAX**  

âœ… **Example DAX Measure: Total Sales per Month**
```DAX
Total_Sales = SUM('processed_sales'[Total_Price])
```
âœ… **Example DAX Measure: Sales Growth Rate**
```DAX
Sales_Growth = 
    DIVIDE(
        [Total_Sales] - CALCULATE([Total_Sales], PREVIOUSMONTH('processed_sales'[Order_Date])),
        CALCULATE([Total_Sales], PREVIOUSMONTH('processed_sales'[Order_Date])),
        0
    )
```
---

## **3ï¸âƒ£ Building the Dashboard**
We need **at least two tiles**, so letâ€™s create **two key visualizations**.

### **ðŸ“Œ Tile 1: Sales Distribution by Product Category**
âœ… **Chart Type:** Bar Chart  
âœ… **Purpose:** Shows which product categories generate the most sales.  
ðŸ“Œ **Steps**:  
1ï¸âƒ£ Drag **Product_Category** to the X-axis  
2ï¸âƒ£ Drag **Total_Sales** to the Y-axis  
3ï¸âƒ£ Apply sorting by **descending order**  
4ï¸âƒ£ Format with a title and labels  

ðŸ” **Example Insight:**  
*â€œElectronics contribute to 40% of total sales, making it the highest-selling category.â€*

---

### **ðŸ“Œ Tile 2: Monthly Sales Trend Over Time**
âœ… **Chart Type:** Line Chart  
âœ… **Purpose:** Tracks sales growth over time.  
ðŸ“Œ **Steps**:  
1ï¸âƒ£ Drag **Order_Date** to the X-axis  
2ï¸âƒ£ Drag **Total_Sales** to the Y-axis  
3ï¸âƒ£ Format the X-axis to **"Month-Year"**  
4ï¸âƒ£ Add a **trendline** for better insights  

ðŸ” **Example Insight:**  
*â€œSales peak in November and December due to Black Friday and holiday shopping.â€*

---

## **4ï¸âƒ£ Publishing & Sharing the Dashboard**
After creating the dashboard, we need to **share it** with stakeholders.

ðŸ“Œ **Steps to Publish**:  
1ï¸âƒ£ Click **"Publish"** in Power BI  
2ï¸âƒ£ Choose **Power BI Service (Online)**  
3ï¸âƒ£ Create a **workspace** and upload your report  
4ï¸âƒ£ Set up **data refresh schedule** (if using DirectQuery)  
5ï¸âƒ£ Share access with **team members or stakeholders**  

âœ… **Automating Refresh (Optional)**:  
- If using **Import Mode**, schedule daily/weekly refresh  
- If using **DirectQuery**, the data updates automatically  

---

## **ðŸŽ¯ Summary of Step 3**
âœ” **Connected Power BI to BigQuery**  
âœ” **Transformed data & created measures in Power BI**  
âœ” **Built two visualizations (Sales by Category & Monthly Sales Trend)**  
âœ” **Published and shared the dashboard**  

---

# **ðŸš€ Next Steps**
âœ… Now that the **dashboard is ready**, do you want to explore **automating Power BI refreshes** or **optimizing the data model further**? Let me know how youâ€™d like to proceed!

# **ðŸš€ Automating Power BI Refresh & Optimizing Data Model**  

Now that the **dashboard is built**, weâ€™ll focus on:  
âœ… **Automating Data Refresh** â€“ Ensuring the Power BI report always has the latest data.  
âœ… **Optimizing Data Model** â€“ Improving performance and reducing query costs.

---

## **ðŸ› ï¸ 1ï¸âƒ£ Automating Power BI Data Refresh**  

### **Option 1: Scheduled Refresh in Power BI Service (for Import Mode)**
If we **imported data** from BigQuery, we need to schedule periodic refreshes.

ðŸ“Œ **Steps to Enable Scheduled Refresh:**  
1ï¸âƒ£ Go to **Power BI Service** â†’ Select your **Published Report**  
2ï¸âƒ£ Click on **Dataset Settings**  
3ï¸âƒ£ Under **Gateway & Cloud Connection**, ensure the Google BigQuery connection is set up  
4ï¸âƒ£ Enable **"Scheduled Refresh"**  
5ï¸âƒ£ Set the refresh frequency (**Hourly, Daily, Weekly**)  

âœ… **Best Practice:** Set refresh to **every 6-12 hours** to balance performance & cost.

---

### **Option 2: Using DirectQuery (for Real-time Updates)**
If we **used DirectQuery**, data updates automatically without needing scheduled refreshes.

âœ… **Pros:**  
âœ” No need for periodic refreshes.  
âœ” Always fetches live data from BigQuery.  

âœ… **Cons:**  
âŒ Slower performance since queries run in real-time.  
âŒ BigQuery costs may increase due to frequent queries.  

ðŸ“Œ **When to Use DirectQuery?**  
Use DirectQuery if **data needs to be updated in real-time** (e.g., stock market dashboards, live sales reports).

---

### **Option 3: Automating Refresh with Power Automate**  
For **more control over refresh timing**, we can use **Power Automate**.

ðŸ“Œ **Steps to Set Up Power Automate for Refresh**  
1ï¸âƒ£ Go to **Power Automate** â†’ Click **New Flow**  
2ï¸âƒ£ Choose **Scheduled Cloud Flow**  
3ï¸âƒ£ Select **Power BI â†’ Refresh Dataset** action  
4ï¸âƒ£ Choose the **Power BI workspace & dataset**  
5ï¸âƒ£ Set **trigger schedule** (e.g., every 4 hours)  
6ï¸âƒ£ Click **Save & Activate**  

âœ… **Benefit**: More flexibility in scheduling refreshes (e.g., trigger refresh **only when new data is added**).

---

## **ðŸ“Š 2ï¸âƒ£ Optimizing Power BI Data Model**  
To **improve performance and reduce costs**, weâ€™ll optimize **data structure & queries**.

### **ðŸ“Œ Best Practices for Data Modeling**
âœ” **Use Aggregations** â€“ Instead of detailed raw data, pre-aggregate totals in BigQuery.  
âœ” **Reduce Column Count** â€“ Only import the **necessary fields**.  
âœ” **Optimize Relationships** â€“ Use **star schema** instead of direct table joins.  
âœ” **Partition & Cluster BigQuery Tables** â€“ Improves query speed.  

âœ… **Example: Optimize a Query in BigQuery**  
Before (Inefficient Query ðŸ›‘ â€“ Scans Entire Table):
```sql
SELECT * FROM `ecommerce_dataset.sales_data`
WHERE Order_Date BETWEEN '2024-01-01' AND '2024-03-31'
```
After (Optimized Query âœ… â€“ Uses Partitioning & Clustering):
```sql
SELECT Order_ID, Customer_ID, Total_Price
FROM `ecommerce_dataset.sales_data`
WHERE Order_Date >= DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH)
```
ðŸ“Œ **Why?**  
- Uses **partitioning** â†’ Queries only **recent data**, reducing scan cost.  
- Limits columns â†’ Only retrieves what Power BI needs.  

---

### **ðŸ“Œ Reducing Query Load on BigQuery**
Since Power BI can query BigQuery **frequently**, we need to **optimize performance**.

âœ… **Strategies**:
âœ” **Use Import Mode for Large Datasets** (Avoids running queries every time a user loads the dashboard).  
âœ” **Cache Data in Power BI** â€“ Reduces calls to BigQuery.  
âœ” **Create Materialized Views in BigQuery** â€“ Precompute reports for faster performance.  

---

## **ðŸŽ¯ Summary & Next Steps**
âœ” **Enabled Automated Data Refresh** (Scheduled Refresh, DirectQuery, Power Automate).  
âœ” **Optimized Power BI Data Model** (Better queries, fewer columns, aggregations).  
âœ” **Reduced BigQuery Costs** (Partitioning, Materialized Views, Import Mode).  

ðŸš€ **Next Step:**  
Would you like to set up **CI/CD automation for deploying Power BI reports** or optimize **BigQuery cost further**? Let me know how you want to proceed! ðŸ˜Š








